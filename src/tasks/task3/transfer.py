"""Transfer learning pipeline for adapting the source-domain model to the target domain."""
from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Mapping, Optional, Sequence

import logging

import numpy as np
import pandas as pd
from sklearn.base import clone
from sklearn.pipeline import Pipeline

from ...analysis.feature_analysis import compute_domain_alignment_metrics
from ...analysis.feature_analysis import run_tsne  # re-export convenience
from ...modeling import SourceDiagnosisConfig, TrainingResult, train_source_domain_model
from .features import TimeFrequencyConfig, extract_time_frequency_features
from .segment_loader import SegmentFetcher

LOGGER = logging.getLogger(__name__)


@dataclass
class PseudoLabelConfig:
    """Configuration governing pseudo-labelling on target data."""

    enabled: bool = True
    confidence_threshold: float = 0.95
    max_iterations: int = 2
    max_ratio: float = 0.4  # Relative to the number of labelled source samples


@dataclass
class TransferConfig:
    """Aggregate configuration for the transfer learning pipeline."""

    diagnosis: SourceDiagnosisConfig
    time_frequency: TimeFrequencyConfig = field(default_factory=TimeFrequencyConfig)
    pseudo_label: PseudoLabelConfig = field(default_factory=PseudoLabelConfig)
    metadata_columns: Sequence[str] = ("file_id", "channel", "segment_index", "start_sample", "end_sample", "rpm")


@dataclass
class TransferResult:
    """Container bundling all artefacts generated by the transfer pipeline."""

    base_result: TrainingResult
    final_pipeline: Pipeline
    feature_columns: List[str]
    source_features: pd.DataFrame
    target_features: pd.DataFrame
    combined_before: pd.DataFrame
    combined_aligned: pd.DataFrame
    alignment_before: pd.DataFrame
    alignment_after: pd.DataFrame
    initial_predictions: pd.DataFrame
    final_predictions: pd.DataFrame
    pseudo_labels: pd.DataFrame
    pseudo_label_history: List[Dict[str, float]]
    time_frequency_features: List[str]


def _infer_sampling_rate(row: Mapping[str, any]) -> float:
    sampling_rate = row.get("sampling_rate")
    try:
        rate = float(sampling_rate)
        if np.isfinite(rate) and rate > 0:
            return rate
    except Exception:
        pass
    length = row.get("segment_length")
    duration = row.get("segment_duration")
    try:
        length_val = float(length)
        duration_val = float(duration)
        if np.isfinite(length_val) and np.isfinite(duration_val) and duration_val > 0:
            return length_val / duration_val
    except Exception:
        pass
    return 1.0


def _augment_with_time_frequency(frame: pd.DataFrame, config: TransferConfig, fetcher: SegmentFetcher) -> pd.DataFrame:
    if frame.empty:
        return frame.copy()

    features: List[Dict[str, float]] = []
    missing_segments = 0
    for _, row in frame.iterrows():
        segment = fetcher.get_segment(row)
        if segment is None or len(segment) == 0:
            features.append({})
            missing_segments += 1
            continue
        sampling_rate = _infer_sampling_rate(row)
        tf_features = extract_time_frequency_features(segment, sampling_rate, config.time_frequency)
        features.append(tf_features)
    if missing_segments:
        LOGGER.warning("Time-frequency features skipped for %s segments due to missing data", missing_segments)
    tf_frame = pd.DataFrame(features)
    return pd.concat([frame.reset_index(drop=True), tf_frame], axis=1)


def _ensure_dataset_column(frame: pd.DataFrame, value: str) -> pd.DataFrame:
    if "dataset" not in frame.columns:
        frame = frame.copy()
        frame["dataset"] = value
    return frame


def _set_target_statistics(pipeline: Pipeline, feature_matrix: np.ndarray) -> None:
    aligner = pipeline.named_steps.get("aligner")
    imputer = pipeline.named_steps.get("imputer")
    if imputer is None:
        X_target = feature_matrix
    else:
        X_target = imputer.transform(feature_matrix)
    if aligner is None or not getattr(aligner, "enabled", False):
        return
    mean = np.mean(X_target, axis=0)
    if X_target.shape[0] > 1:
        covariance = np.cov(X_target, rowvar=False)
    else:
        covariance = np.eye(X_target.shape[1])
    aligner.set_target_statistics(mean, covariance)


def _transform_aligned(pipeline: Pipeline, feature_matrix: np.ndarray) -> np.ndarray:
    imputer = pipeline.named_steps.get("imputer")
    aligner = pipeline.named_steps.get("aligner")
    X = feature_matrix
    if imputer is not None:
        X = imputer.transform(X)
    if aligner is not None and getattr(aligner, "enabled", False):
        X = aligner.transform(X)
    return X


def _select_metadata(frame: pd.DataFrame, columns: Sequence[str]) -> pd.DataFrame:
    available = [column for column in columns if column in frame.columns]
    return frame[available].reset_index(drop=True)


def _predict_with_pipeline(
    pipeline: Pipeline,
    frame: pd.DataFrame,
    feature_columns: Sequence[str],
    metadata_columns: Sequence[str],
) -> pd.DataFrame:
    X = frame[feature_columns].astype(float)
    metadata = _select_metadata(frame, metadata_columns)
    metadata = metadata.reset_index(drop=True)
    predicted = pipeline.predict(X)
    result = metadata.copy()
    result["row_index"] = frame.index.to_numpy()
    result["predicted_label"] = predicted
    classifier = pipeline.named_steps.get("classifier")
    if classifier is not None and hasattr(classifier, "predict_proba"):
        proba = pipeline.predict_proba(X)
        classes = getattr(classifier, "classes_", [])
        for idx, class_name in enumerate(classes):
            result[f"probability_{class_name}"] = proba[:, idx]
        result["max_probability"] = proba.max(axis=1)
    return result


def _apply_pseudo_labelling(
    base_pipeline: Pipeline,
    feature_columns: Sequence[str],
    source_frame: pd.DataFrame,
    target_frame: pd.DataFrame,
    label_column: str,
    metadata_columns: Sequence[str],
    config: PseudoLabelConfig,
) -> Tuple[Pipeline, pd.DataFrame, List[Dict[str, float]]]:
    classifier = base_pipeline.named_steps.get("classifier")
    if not config.enabled or classifier is None or not hasattr(classifier, "predict_proba"):
        return base_pipeline, pd.DataFrame(), []

    X_source = source_frame[feature_columns].astype(float)
    y_source = source_frame[label_column].astype(str)
    X_target = target_frame[feature_columns].astype(float)

    current_pipeline = base_pipeline
    pseudo_records: List[pd.DataFrame] = []
    history: List[Dict[str, float]] = []
    used_mask = np.zeros(len(target_frame), dtype=bool)
    total_source = len(source_frame)
    if config.max_ratio > 0:
        max_total = max(int(round(config.max_ratio * total_source)), 1)
    else:
        max_total = None
    total_selected = 0

    for iteration in range(config.max_iterations):
        proba = current_pipeline.predict_proba(X_target)
        predictions = current_pipeline.predict(X_target)
        max_proba = proba.max(axis=1)
        candidate_mask = (max_proba >= config.confidence_threshold) & (~used_mask)
        candidate_indices = np.where(candidate_mask)[0]
        if candidate_indices.size == 0:
            break
        if max_total is not None:
            remaining = max_total - total_selected
            if remaining <= 0:
                break
            candidate_indices = candidate_indices[:remaining]
        selected_rows = target_frame.iloc[candidate_indices].copy()
        selected_rows[label_column] = predictions[candidate_indices]
        selected_rows["pseudo_probability"] = max_proba[candidate_indices]
        selected_rows["pseudo_iteration"] = iteration + 1
        selected_rows["dataset"] = "target_pseudo"
        pseudo_records.append(selected_rows)
        used_mask[candidate_indices] = True
        total_selected += len(candidate_indices)

        combined = pd.concat([source_frame, *pseudo_records], ignore_index=True)
        X_combined = combined[feature_columns].astype(float)
        y_combined = combined[label_column].astype(str)
        next_pipeline = clone(base_pipeline)
        next_pipeline.fit(X_combined, y_combined)
        _set_target_statistics(next_pipeline, X_target)
        current_pipeline = next_pipeline

        history.append(
            {
                "iteration": float(iteration + 1),
                "new_samples": float(len(candidate_indices)),
                "cumulative_pseudo": float(total_selected),
                "threshold": float(config.confidence_threshold),
                "probability_mean": float(np.mean(max_proba[candidate_indices])),
                "probability_min": float(np.min(max_proba[candidate_indices])),
                "probability_max": float(np.max(max_proba[candidate_indices])),
            }
        )

        if max_total is not None and total_selected >= max_total:
            break

    pseudo_df = pd.concat(pseudo_records, ignore_index=True) if pseudo_records else pd.DataFrame()
    return current_pipeline, pseudo_df, history


def run_transfer_learning(
    source_features: pd.DataFrame,
    target_features: pd.DataFrame,
    config: TransferConfig,
) -> TransferResult:
    LOGGER.info("Augmenting features with time-frequency descriptors ...")
    fetcher = SegmentFetcher()
    augmented_source = _augment_with_time_frequency(source_features, config, fetcher)
    augmented_target = _augment_with_time_frequency(target_features, config, fetcher)

    augmented_source = _ensure_dataset_column(augmented_source, "source")
    augmented_target = _ensure_dataset_column(augmented_target, "target")

    base_result = train_source_domain_model(augmented_source, config.diagnosis)
    feature_columns = base_result.feature_columns

    target_matrix = augmented_target[feature_columns].astype(float)
    _set_target_statistics(base_result.pipeline, target_matrix)

    initial_predictions = _predict_with_pipeline(
        base_result.pipeline,
        augmented_target,
        feature_columns,
        config.metadata_columns,
    )

    label_column = config.diagnosis.label_column
    source_visual = augmented_source[feature_columns].reset_index(drop=True)
    source_labels = augmented_source.get(label_column)
    if source_labels is not None:
        visual_labels = source_labels.fillna("未标注").astype(str)
    else:
        visual_labels = pd.Series(["未标注"] * len(source_visual))
    source_visual["dataset"] = "source"
    source_visual["label"] = visual_labels.values

    target_visual_base = augmented_target[feature_columns].reset_index(drop=True)
    target_visual_base["dataset"] = "target"
    initial_labels = initial_predictions.get("predicted_label", pd.Series(["未知"] * len(target_visual_base)))
    initial_labels = initial_labels.astype(str)
    target_visual_before = target_visual_base.copy()
    target_visual_before["label"] = [f"预测(初始):{label}" for label in initial_labels]

    combined_before = pd.concat([source_visual, target_visual_before], ignore_index=True)
    alignment_before = compute_domain_alignment_metrics(combined_before)

    final_pipeline = base_result.pipeline
    pseudo_labels = pd.DataFrame()
    pseudo_history: List[Dict[str, float]] = []

    if config.pseudo_label.enabled:
        LOGGER.info("Applying pseudo-labelling strategy with threshold %.2f", config.pseudo_label.confidence_threshold)
        final_pipeline, pseudo_labels, pseudo_history = _apply_pseudo_labelling(
            base_result.pipeline,
            feature_columns,
            augmented_source,
            augmented_target,
            config.diagnosis.label_column,
            config.metadata_columns,
            config.pseudo_label,
        )

    final_predictions = _predict_with_pipeline(
        final_pipeline,
        augmented_target,
        feature_columns,
        config.metadata_columns,
    )

    final_labels = final_predictions.get("predicted_label", pd.Series(["未知"] * len(target_visual_base)))
    final_labels = final_labels.astype(str)
    target_visual_after = target_visual_base.copy()
    target_visual_after["label"] = [f"预测(对齐):{label}" for label in final_labels]

    final_aligned_source = _transform_aligned(final_pipeline, augmented_source[feature_columns].astype(float))
    final_aligned_target = _transform_aligned(final_pipeline, target_matrix)
    final_source_df = pd.DataFrame(final_aligned_source, columns=feature_columns)
    final_source_df["dataset"] = "source_aligned"
    final_source_df["label"] = source_visual["label"].values
    final_target_df = pd.DataFrame(final_aligned_target, columns=feature_columns)
    final_target_df["dataset"] = "target_aligned"
    final_target_df["label"] = target_visual_after["label"].values
    combined_aligned = pd.concat([final_source_df, final_target_df], ignore_index=True)
    alignment_after = compute_domain_alignment_metrics(combined_aligned)

    time_frequency_features = [column for column in augmented_source.columns if column.startswith("tf_")]

    return TransferResult(
        base_result=base_result,
        final_pipeline=final_pipeline,
        feature_columns=list(feature_columns),
        source_features=augmented_source,
        target_features=augmented_target,
        combined_before=combined_before,
        combined_aligned=combined_aligned,
        alignment_before=alignment_before,
        alignment_after=alignment_after,
        initial_predictions=initial_predictions,
        final_predictions=final_predictions,
        pseudo_labels=pseudo_labels,
        pseudo_label_history=pseudo_history,
        time_frequency_features=time_frequency_features,
    )


__all__ = [
    "PseudoLabelConfig",
    "TransferConfig",
    "TransferResult",
    "TimeFrequencyConfig",
    "run_transfer_learning",
    "run_tsne",
]
